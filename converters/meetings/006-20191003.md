# Thursday Oct 03, 2019 at 10:00-11:00am PST
* online: https://ibm.webex.com/join/chhuang
* call-in: 1-844-531-0958, 927 169 571 #

## Agenda
* Greetings! 
* Discussion topics
    * Training proposal update and planning
    * ML ops tests
    * Inputs with unknown ranks use cases
    * Converter dashboard update
    * Support different naming rules 

## Attendees 
* Chin Huang (onnx-tf, IBM)
* Guenther Schmuelling (tf-onnx, Microsoft)
* Winnie Tsang (onnx-tf, IBM)
* Tom Truong (IBM)
* Emma Ning (mltools, Microsoft)
* Spandan Tiwari (pytorch, Microsoft)
* Jason Plurad (IBM)

## Notes
* We ended up discussing the training proposal for the entire session today. The following are key discussions, questions and answers:
    * Why ONNX training and what role is there for converters? Some members don't know the use cases for ONNX training. Transfer learning and interoperable training are toptwo use cases. Converters play a critical role of providing and consuming the ONNX training model from and to various frameworks. 
    * What is the current state of ONNX training and what is expected to be fully functional in converters at the ONNX 1.7 release time? The ONNX training PR 2314 includes new schema, code, docs, and tests. Additionally a test script is provided to artificially create onnx training files. However, both Spandan and Guenther raised issues in unclear user experience, lack of frontend design, and potential technical difficuties such as training loops, optimizers, data augmentation, and constant folding. The backend converter also will need to deal with function proto, TrainingInfo proto, loss functions, optimizers, and gradients. It requires a lot time for converters to formally verify and then support the training scenario. As the proposal still evolving, we do not foresee converters to be able to support training at the ONNX 1.7 release time.
    * Spandan has looked into the PR in reasonable detail. There are still important open questions: what is the end to end user experience? what does it mean for an ONNX model to be trainable? To enable training in Pytorch exporter is going to be a big task. Training loop and optimizers cannot be captured in torch IR which is the first step in the conversion process. Loss functions are lesser of a problem. Additional questions, for a lot exporters, how do we decide which parameters are trainable? If initializer is not connected a graph input, is it a constant? Or a variable? Or somewhere in between? Does a training graph must have a backward graph with gradient operaters? With all these questions, Pytorch exporter is not ready even for a POC in the 1.7 time frame. In export scenario, we should allow some user code change in training.
    * Guenther stated training in Tensorflow exporter is not a trivial task. For loss functions you should be able to pass a sub graph, not limited to the two in the PR. Maybe just say it is a function and fill in with sub graph? In training you also have the option to choose an optimizer, a scenario that the user could provide an optimizer and the frontend converter knows how to convert to ONNX. There is no data augmentation ops in ONNX also a concern.
    * Chin commented we might want to take an optimizer like Adam and see how we can produce the node and attributes from Pytorch and Tensorflow. By doing some prototype code, not formally in the converter code base, will help to drive out the gaps in the current training proposal and evolve into the first implementation for other converters to follow.
    * Are converters on the critical path to provide the first training tutorial for ONNX 1.7? No, as summarized above. Guenther mentioned that ONNX 1.0 released without any converters. And we suggest to apply the same approach here, ie, ONNX 1.7 could release with training spec without converters.
    * What about the end-to-end use case from Pytorch to ONNX to Tensorflow for training Mnist? Spandan would like to see clear user experience documented, and asks if we can just use the test script for end to end. Guenther wants to have a solid design before coding, and concerns about any POC code being wasted. Chin stated for the onnx-tf backend converter, taking the hard coded script as training model input is simply not end-to-end. The linear model prototype done earlier should serve the purpose for basic design validation. The real model end-to-end training between frameworks is important and we will work on the details after 1.7.
    * What are the immediate actions to help move training PR 2314 forward? For now, we will review and make comments on the training PR. Also help prototyping specific scenario as needed.
* We will provide our group summary and recommendations in the training WG meeting in two weeks.

## Action items
* 1: Investigate the feasibility of supporting ONNX training from tf and pytorch and Tensorflow
* 2: Investigate and eventually propose a good design for frontend converters to work with ONNX training
* 3: Reveiw and comment on the traininig proposal
