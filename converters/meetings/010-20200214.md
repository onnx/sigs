# Thursday Feb 14, 2020 10:30-11:30am PST
* https://zoom.us/j/7376656864

## Agenda
* Greetings! 
* Discussion topics
    * Training proposal update and planning (use cases, new global_mutable_initializer_name)
    * ONNX 1.7
    * Support mulitple versions of a framework 
    * Inputs with unknown shapes unit tests and effects
    * Sequence ops handling in backend converters
    * Channel first vs channel last
    * Revisit onnx-ml operators
    * Other topics

## Attendees 
* Chin Huang (onnx-tf, IBM)
* Guenther Schmuelling (tf-onnx, Microsoft)
* Jonny Shipton (Myrtle.ai)
* Svetlana Lavitan (IBM)
* Thomas Truong (IBM)
* Alexandre Eichenberger (onnx mlir, IBM)

## Notes
* We identified two uses for training: 1. the onnx model contains inference only graph and the backend converters/runtimes will generate training graph 2. the onnx model contains inference and training information, such as optimizers, loss functions, and gradient operators. That means the frontend converters need to produce TrainingInfoProto and the backend will simply consume the training data without auto-gen.
* The ONNX 1.7 release is targeted early March with the new TrainingInfoProto and global_mutable_initializer_name in the spec for training as a tech preview. The current focus is on the spec and ONNX Runtime to verify the use case 1 above. Deep 500 was mentioned as possible framework to help. We recommend the converter teams to get familiar with the training design and spec and start working on training support such as how to generate gradient operators in future releases.
* The ONNX backend unit test doesn't cover unknown tensor shapes as inputs and has limited tests for various data types. We will bring this concern toi ONNX core community and collaborate on a more complete backend test suite.
* The ONNX sequence operators can be converted to tf.RaggedTensor in Tensorflow. We saw the test code in model test and would like to learn the practical use cases to understand and verify the functionality.
* Handling channel first (NCHW) vs channel last (NHWC) is different between frameworks. With Tensorflow as a backend, we could either 1. always convert to NHWC and let TF do the additional conversion based on physical devices if needed 2. convert data format for a target runtime (GPU vs CPU). Since certain transpose ops will be added to the graph, it is adviced to include some optimization to minimize the graph. This topic seems most relevant only for image models.
* ONNX-ML operators are used in quite a few frameworks and ONNX Runtime. TF frontend started to make use of them. TF backend will also look into the ML operator support.
* Alexandre introduced the MLIR conversion and discussed briefly the current challenges. We would like to continue the discussion and provide summary in next meeting.

## Action items
* 1: Identify and design ONNX training support for frontend and backend converters.
* 2: Investigate and share support for the ONNX-ML operators
