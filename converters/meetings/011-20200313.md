# Friday March 13, 2020 10:30-11:30am PST
* https://zoom.us/j/7376656864

## Agenda
* Greetings! 
* Discussion topics
    * Training proposal update and planning (merged version review)
    * ONNX 1.7 update
    * Inputs with unknown shapes 
    * Loop operator support in backend
    * Onnx-mlir updates
    * PMML to ONNX-ML converter?

## Attendees 
* Chin Huang (onnx-tf, IBM)
* Guenther Schmuelling (tf-onnx, Microsoft)
* Kevin Chen (Nvidia)
* Winnie Tsang (onnx-tf, IBM)

## Notes
* Chin reviewed the merged ONNX training addition, TrainingInfoProto includinng 1. training algorithm as a GraphProto 2. initialization algorithm also a GraphProto 3. initialization binding as a text to text mapping that indicates which initializers to update
* We continued to discuss two use cases for training with a number of unanswered questions. We would like to take them to the training WG and share with broader converter community for discussions.
* Case 1: the onnx model contains inference only graph and the backend converters/runtimes will generate training graph
    * Runtime executes training with defaults for hyperparameters, loss function, optimizer?
    * Runtime persists a ONNX trained model after n iterations? Allow change of graph or hyperparameters by users? The assumption is yes. How to change loss function, remove/add a layer? Guenther said there is some configuration file for ONNXruntime. We would like to identify the scenario for all runtimes.
    * Backend converter converts the inference only model to a trainable model in a framework, such as TF?
    * Backend converter executes training with defaults for hyperparameters, loss function, optimizer?
    * Is it going to be a train_model API for onnx base Backend?
    * Backend converter persists a ONNX trained model after n iterations?
* Case 2: the onnx model contains inference and training information, such as optimizers, loss functions, and gradient operators
    * Frontend converter generates the training info as described in spec, such as hyperparameters, training initialization, algorithm, gradients, loss function, optimizer
    * Runtime executes training as described in the model/training info
    * Runtime persists a ONNX trained model after n iterations?
    * Backend converter converts to a trainable model in a framework, such as TF
    * Backend converter executes training as described in the model/training info
    * Backend converter persists a ONNX trained model after n iterations?
* One key question is how many converters are going to support ONNX traininig and at which stage? Guenther said the tf-onnx frontend converter has not started anything and don't have a plan. Chin commented the onnx-tf backend converter is in very early stage of investigation and design. Would love to learn about others.
* The ONNX 1.7 release has opset=12 (8 new ops) and training opset=1 (2 new ops).
* Kevin asked for clarification on frontend vs backend converters. A simple way to define, frontend converters convert models from a framework format to ONNX, and backend has two types: 1. runtimes execute on ONNX models directly 2. converters convert from ONNX format to framework for inference and training in the framework.
* We did not go over other topics on the agenda due to lack of representation in these areas. Will continue in next meeting.

