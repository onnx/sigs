<!--- SPDX-License-Identifier: Apache-2.0 -->

# Friday May 22, 2020 10:30-11:30am PST
* https://zoom.us/j/7376656864

## Agenda
* Greetings!
* Discussion topics
    * Converters project updates
    * ONNX release 1.7
    * Resize operator review
    * Partial operator support fall back to ONNX runtime
    * Range operator model test output has no data types
    * Unknown rank inputs (SplitToSequence)


## Attendees
* Chin Huang (onnx-tf, IBM)
* Guenther Schmuelling (tf-onnx, Microsoft)
* Kevin Chen (Nvidia)
* Winnie Tsang (onnx-tf, IBM)
* Ting Su (MathWorks)
* Svetlana Levitan (IBM)
* Alexandre Eichenberger (onnx mlir, IBM)

## Notes
* ONNX-TF converter: working on TF 2.0 native support and ONNX opset 11
* TF-ONNX converter: support of ONNX 1.7 and opset 12 should be out in next week or two, exploring use of TF function to convert to ONNX ops
* MatLab converters: working on ONNX opset 10 and 11, will look at opset 12 soon
* Tensor-RT converter: no immediate plan to support training, will look into other ops in opset 12 once things settle down
* ONNX-MLIR: has 50+ operators, not supporting return of indices, seeing a challenge to implement ops with many definitions, adding end-to-end testing for model zoo, not looking into performance yet
* ONNX 1.7 is releaseed with opset 12 and training in a preview domain
* ONNX traininig is adding an indicator to inputs and outouts to be differentable or not
* Resize opset 11 added new attributes and logic. MatLab implements with own APIs whenever available or puts a placeholder for custom code, has issues with scales being empty string, ONNX-TF and Tensor-RT have partial support certain modes, common modes for Nvidia, half_pixel, pytorch_half_pixel, align_cornors and asymmetric, recommend to figure out the reasons why other options are introduced, for specific models? We all feel using sizes instead of scales is the easier approach for both frontend and backend converters.
* Tensor-RT falls back to ONNX runtime for graph partitioning and unsupported ops/attributes. ONNX runtime will do the execution where the subgraphs can't run on a particular runtime. Not looking possible for Tensorflow to integrate with ONNX runtime at this time. Maybe open an PR in Tensorflow.
* Are TF converters using TF or/and TFRT (Tensorflow Runtime)? Currently TF, will look into TFRT.
* Range op test in ONNX has a loop body with no output data types only names. The output of a subgraph in a loop can be unspecified? Could output types be inferred from inputs? Would like to learn how other backend converters/runtimes pass the model test.
* During modeling time, can an ONNX model include a node input with unknown rank? SplitToSequence split input could be a scalar or a 1-d tensor. Need to check if ONNX allows unknown rank in tensor proto creation.
