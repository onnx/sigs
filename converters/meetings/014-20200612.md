# Friday June 12, 2020 10:30-11:30am PST
* https://zoom.us/j/7376656864

## Agenda
* Greetings! 
* Discussion topics
    * Converters project updates
    * Subgraph with no output data types (Range)
    * Inputs with unknown ranks update
    * Optional inputs and outputs (Clip, Compress, Slice, NonMaxSuppression, ConvIntegerâ€¦)
    * Others

## Attendees 
* Chin Huang (onnx-tf, IBM)
* Guenther Schmuelling (tf-onnx, Microsoft)
* Kevin Chen (Nvidia)
* Winnie Tsang (onnx-tf, IBM)

## Notes
* ONNX-TF converter: support of ONNX opset 11 and release 1.6 will be out in a couple of weeks 
* TF-ONNX converter: recently completed support for ONNX 1.7 and opset 12
* Tensor-RT converter: working on opset 12 ops, no immediate plan to support training ops
* Custom ops are created from tf-onnx and can be handled by ONNX runtime. How other runtimes and frameworks would work? Currently onnx-tf and tensor-rt focus on standard onnx ops and do not handle custom ops.
* Subgraph with no output data types doesn't work in onnx-tf. Would like to understand how tensor-rt manages the standard backend test.
* Inputs with unknown ranks can be created from API but will cause an exception from the onnx checker. So we believe unknown ranks are not supported in onnx.
* Optional inputs can be provided in two ways according to the IR.md spec. The use of an empty string in place of an input and output name was discussed. The tf-onnx has not applied this method. The tensor-rt and onnx-tf have not supported this case, nor seen any models with such input or output names. The conclusion and recommendation from today's discussion is to use the number of inputs to decide whether optional ipnuts are actually in use. The models need to have default values for the optional inputs in the middle positions when there is a trailing optional input with a real runtime value. That means the backends can expect all inputs specified for a node will be filled with data at the runtime.
* We recommend to revisit the method of using an empty string as input/output name at ops SIG meeting.
* We will continue optional outputs discussion in next meeting.
