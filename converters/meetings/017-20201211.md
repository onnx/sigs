<!--- SPDX-License-Identifier: Apache-2.0 -->

# Friday Dec 11, 2020 10:30-11:30am PST
* https://zoom.us/j/7376656864?pwd=SjdiaGdvUFFyVXM4OUxTWndYZ2Z1Zz09

## Agenda
* Greetings!
* Discussion topics
    * Converters project updates
    * Custom ops story
    * Adding NHWC option to specific ops
    * Adding data processing to ONNX
    * Conversion verification using model zoo models

## Attendees
* Chin Huang (onnx-tf, IBM)
* Guenther Schmuelling (tf-onnx, Microsoft)
* Winnie Tsang (onnx-tf, IBM)
* Sarah Mohamed (matlab)
* Kevin Chen (tensor-rt, Nvidia)

## Notes
* onnx-tf updates: release 1.7 is completed, supporting Tensorflow 1 (will be discontinued) and 2, added a model zoo report to wiki, currently with 98% successful conversion rate, working on the inference verification
* tf-onnx updates: almost done with opset 13, prototype for TF Lite, quantization almost working
* tensor-rt updates: working on more operator coverage, patching up to pass at least 70% ONNX backend test, made progress in dynamic shape support, two ways to support an onnx op, using tensor-rt native API or through a plugin
* matlab updates: import onnx model to matlab code, complete coverage model zoo, working on new models
* We should be aware of issues and coordinate between tf-onnx and onnx-tensorrt
* The custom op plugin project for ONNX runtime is available at https://github.com/microsoft/ort-customops. For backend for existing frameworks, we need to uunderstand the custom op at the source and convert accordingly.
* Does it make sense to have the option of keeping NHWC format in onnx model, since some frameworks and devices without GPUs are channel last by nature? It will cause additional work for backend converters to deal with the new attribute. Should open an issue for community feedback.
* Data processing, in particular DataFrame and related functions in pandas, could be a nice addition to ONNX. This means the source could come from multiple frameworks, such as pandas and Tensorflow. And all operations are converted into one ONNX graph. There must be some way to differentiate data processing from model inference in onnx model so some backends could properly process certain portion. Pandas has a lot of functions. Design needs to be flexible and well thought out.
* Tensor-rt is focusing on final step of model inference in a pipeline, not training or data processing.
* How does a backend verify against model zoo inference? What is a reasonable tolerance threshold? Tensor-rt doesn't use the sample inputs and outputs from model zoo. It uses some random data and compares results with ONNX runtime and other runtimes. Matlab applies the similar methodology of comparing results with other reference runtimes. onnx-tf is attempting to use the provided inputs and outputs in model zoo and get to e-3 absolute and relative accuracy.
* A tool could be used to verify inference results, https://github.com/NVIDIA/TensorRT/tree/master/tools/Polygraphy
