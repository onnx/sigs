# Friday Aug 20 2021 10:30-11:30 pm PST
* https://zoom.us/j/7376656864?pwd=SjdiaGdvUFFyVXM4OUxTWndYZ2Z1Zz09

## Agenda
* Greetings!
* Discussion topics
    * Converters project updates
      * onnx-tf: released 1.9.0, supporting opset 14, Tensorflow 2.6
      * tensor-rt: focus on operator coverage, will look into participating the ONNX backend scoreboard
      * tf2onnx: keras to ONNX stopped, will be done using tf2onnx, still fixing GRU, developed a layout transformer to convert channel first to channel last in ONNX format, can convert JAX to ONNX
    * How keras models are supported? Keras operators like lstm are lowered to Tensorflow graph. So tf2onnx will match up the sub graphs/patterns and convert them to ONNX models.
    * How to support older versions of IR and opset? Most converters have natural support for older versions. The ONNX version converter has not been robust or up-to-date in the past. ONNX-MLIR would prefer to work with the latest ONNX version, therefore heavily depend on high quality version converter to ensure old versions can all convert to the latest.
    * SparseTensor became a first-class data type in ONNX 1.10, opset 15; it is however applied in only one operator Constant to turn a sparse tensor into a dense one. It can be used to minimize memory as a constent. We are interested to know if more operators would include sparse tensors to make it more useful.
    * Optional is a new data type in ONNX 1.10, opset 15. We are still looking into how to convert the type and operators. Would be nice to see real use cases and models.

## Attendees
* Chin Huang (onnx-tf)
* Guenther Schmuelling (tf2onnx)
* Kevin Chen (Tensor-RT)
* Alexandre Eichenberger (onnx-mlir)
* Charles Volzka

## Notes

