# Tuesday June 24, 2025 at 22 CEST

## Access
* use zoom-link at https://zoom-lfx.platform.linuxfoundation.org/meetings/lfai-onnx?view=month

## Participants
* Christian Bourjau (QuantCo)
* Andreas Fehlner
* Justin Chu (Microsoft)
* Alexandre Eichenberger (IBM)



## Agenda
* 1.) Github Enterprise (Andreas
* 2.) Testing in ONNX (Christian)
* 3.) pixi for faster builds/checks in our CI-pipelines [Christian]
* 4.) Introduce ml_dtypes as a dependency (https://github.com/onnx/onnx/issues/6605) [Justin]

Not discussed:
* Release 1.19, Release Cut in July [Andreas]
* pypi organization

### Action Items / Next Steps ###
* Andreas: Write to the S-ONNX team to discuss potential collaboration on test suite development
* Christian: Continue prototyping to factor out the test data generation from test cases for the conformance test suite
* Justin: Continue migration of custom data types in the reference runtime to use ml_dtypes

### 1. GitHub Enterprise
Github has changed their monetization policy and now their hosting of ONNX project is no longer free. To lower down costs and to keep this community sustainable we are looking at ways to use github resources efficiently.
Preliminary optimizations:
* Changing "member" to "outside collaborator"
* Reducing mac os runner

### 2. Testing in ONNX ###

#### Tensor Library Testing Challenges: ####
The challenges of testing tensor libraries, particularly ONNX, were discussed, with insights drawn from experience with Array-API tests. The vast permutations of possible test cases make handcrafted tests impractical. Parameterized testing was identified as a promising approach, with a proof of concept available. It was noted that while conformance testing is a primary goal, test data could also be used for purposes such as verifying version converters.

#### Testing Framework Structure and Flexibility: ####
The structure and flexibility of the testing framework were discussed, including the vision for separating test data and logic to allow for versatile testing scenarios, such as shape inference and symbolic execution. The current process involves generating randomized inputs, building models, and comparing outputs between a reference and candidate implementation. It was agreed that the framework could be adapted to focus on aspects like model validity or shape inference by modifying the test logic, rather than assuming a fixed two-part comparison structure.

#### Running Tests in non-python environments: ####
The team discussed the challenges of running tests in non-Python environments and explored options for supporting test data generation and conformance testing. The value of a centralized Python test suite for downstream implementations was emphasized, while also suggesting the provision of facilities for users to generate and use test data flexibly. The importance of a standard way to declare conformance was agreed upon, with potential collaboration with a working group on safety for critical systems discussed.

#### Conformance and Operator-Specific Testing: ####
Test suite approaches for ONNX were discussed, focusing on two main types: a comprehensive conformance test suite and targeted operator-specific tests. It was agreed that a conformance test suite should be easy to configure and maintain, potentially generated operator-by-operator, while allowing users to filter and extend tests as needed. The separation of input generation from test cases, with explicit test functions for each operator (similar to the Array-API approach), was suggested, along with the value of allowing users to select specific tests. The potential benefits of combining pytest parameterization with hypothesis sampling for input generation were discussed, though it was noted that explicit test cases might be easier to maintain.

#### Test Suite Integration Discussion: ####
A test suite was discussed with the team, who expressed support for potentially including it in the onnx namespace. Plans were made to discuss the idea with the ONNX team, though scheduling challenges were noted due to time zone differences. Prototyping of the test suite's generation functionality was planned, with updates to be provided to the team. The current testing infrastructure was mentioned, with a suggestion to make the proof of concept more concrete. The need for updating model tests was briefly touched upon, and it was agreed to continue discussions about the test suite's potential placement under the onnx namespace.

### 3. Pixi Integration for ONNX Development: ###
The integration of [Pixi](https://pixi.sh/latest/), a tool for managing development environments, into `onnx` was explained. Pixi's functionality allows for cross-platform package management and easy recreation of development environments. The use of Pixi for unit tests in ONNX was suggested, potentially reducing CI costs and simplifying environment setup. Questions were raised about the availability of packages on Pypi, and it was clarified that while conda-forge packages cannot depend on Pypi packages, both sources can be used in combination. 

### 4. New Data Types (with `ml_dtypes`) for ONNX ###
Progress on migrating custom data types in the ONNX Python package to use ml_dtypes was reported, and it was agreed to proceed with regular releases while incorporating new data types as needed, with plans to include new data types in the next release. The implementation and usage of ML types were discussed, focusing on their handling of lower precision integers and compatibility with operations like matrix multiplication. ML types use an unpacked representation and support basic arithmetic operations, while the onnxruntime handles quantized operations through a dequantize-quantize approach.
