# Thursday March 18, 2021 at 9:00am PST

## Agenda

* ONNX 1.9 release plan: target date: 3/25 (code freeze); 4/12 (release)
* [PR 2625](https://github.com/onnx/onnx/pull/2625): FFT and iFFT ops
* [PR 3332](https://github.com/onnx/onnx/pull/3332): HardSwish op
* [PR 3333](https://github.com/onnx/onnx/pull/3333): BatchNorm training-mode support
* [PR 3334](https://github.com/onnx/onnx/pull/3334): 8 and 16 bit integer math ops


## Attendees 

* Ganesan Ramalingam (Microsoft)
* Michal Karzynski (Intel)
* Scott Cyphers (Lightmatter)
* Spandan Tiwari (Microsoft)
* Ashwini Khade (Microsoft)
* Chun-Wei (Jacky) Chen (Microsoft)
* Matteo Salvarezza (Wolfram Research)
* Sergey Lyalin (Intel)


## Notes

### ONNX 1.9 release plan

* Reviewed [logistics for next release](https://github.com/onnx/onnx/wiki/Logistics-for-ONNX-Release-1.9)
* Decided that it would be prudent to add support for Python 3.9 in this release and drop support for Python 3.5
* Python 3.6 support should be dropped by the end of 2021. We should start communicating this at ONNX Workshops, to give people more time to prepare.
* For the next release, it's recommended to announce the release date earlier, to give more time to review/test peding PRs. 

### FFT/iFFT

* postponed to next SIG meeting.
* ONNX Runtime has an custom FFT operation, we should ask for operator feedback from ORT team.

### HardSwish

* [PR 3332](https://github.com/onnx/onnx/pull/3332) is under review
* Recommended releasing it with ONNX 1.9
* PR now marked for 1.9 milestone, will be tracked for release. 

### BatchNorm

* discussed the training mode for BatchNorm operation. It's significantly different from inference mode, where training statistics are not tracked or output.
* some frameworks have separate BatchNorm and BatchNormTraining operations for this reason.
* recommended: keep single BatchNorm operation, but make clear in the documentation that the values produced by extra outputs are undefined in inference mode. 

### 8 and 16 bit integer math ops

* PR extends arithmetic operations to support 8 and 16 bit precision
* discussed concerns about overflow behaviour. Recommended: overflow behaviour should be explicitly undefined, as it may be hardware specific. 
  Model authors should verify their models are not susceptible to problems caused by overflow.
* Current version of arithmetic operations use the same type for inputs and output. In the future may want to allow the 
  output to have a different precision, specified by an attribute. We may also consider an attribute specifying the 
  precision of operation accumulators.
* In the future, for changes like this one we will not require bumping operator versions. For consistency with other PRs, 
  already scheduled for this release, we recommend bumping the opset versions of operators changed in this PR.

### Opens

Matteo mentioned his [PR which adds missing opset version converters](https://github.com/onnx/onnx/pull/3243).
SIG recommended that this PR be split into 2 PRs: one wich adds missing converter adapters, and another adding tests infrastructure.
The first PR will be a candidate for 1.9 release. The other one imposes a requirement on any new operator changes to 
include a opset converter adapter. We need to discuss this requirement further before we add it.


## Action items
* Michal - Invite the Jérémy Cochoy, author of FFT/iFFT PR to next SIG meeting.
* Rama - ask author of ONNX Runtime FFT operation to review the proposed specification.

