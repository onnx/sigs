# Thursday May 13, 2021 at 9:00am PST

## Agenda

* Opens
* Should bug-fix updates to function-body require an update of the op-version-number? (e.g. [PR 3486](https://github.com/onnx/onnx/pull/3486))
* Use of attributes vs. inputs: eg., ReduceSum expects axes as input, while other Reduce ops expect axes as attribute.
* [PR 3484](https://github.com/onnx/onnx/pull/3484): Accumulate attribute for scatter/gather ops
* [PR 3431](https://github.com/onnx/onnx/pull/3431): New Bernoulli op/function


## Attendees 

* Ashwini Khade (Microsoft)
* Calvin McCarter (Lightmatter)
* Chun-Wei (Jacky) Chen (Microsoft)
* Evgeny Lazarev (Intel)
* Ganesan Ramalingam (Microsoft)
* Michal Karzynski (Intel)
* Scott Cyphers (Lightmatter)
* Sergey Lyalin (Intel)
* Tobias WÃ¼rfl (Siemens)

## Notes

### Discussion of Architecture & Infrastructure topics

Ashwini suggested that we spend some time during the Operators SIG meeting to discuss topics which relate 
to the work of Architecture & Infrastructure SIG. We agreed to so in the future.

 
### Criteria for updating Function operator version numbers

We discussed the criteria for updating operator version number in cases of function operators.
In some cases changes are made to the function body, in other cases changes can be made to operators used
within the function body, which can also affect how the function behaves.

As an example from 1.8 release, Aswini brought up [PR 2955](https://github.com/onnx/onnx/pull/2955), which 
introduced changes in the `Loop` operator. As a result of this we saw model load failures for models using
the `NegativeLogLikelihoodLoss` operator, so it had to be updated in [PR 3059](https://github.com/onnx/onnx/pull/3059).
With that change NLL operator's version number was bumped to 13.

Rama also brought up the problem that function bodies don't specify which operator versions they require. 
Discussion on this subject is started in this [issue](https://github.com/onnx/onnx/issues/3139) and we should
follow up on it in future meetings.

We decided that for very small bugfix changes which are fully backward-compatible, we don't need to require
an operator version number bump. Example: [PR 3486](https://github.com/onnx/onnx/pull/3486)

However, for any breaking change in an operator used by a function or change in function body, which changes
the results it generates, must be accompanied by a version number bump.


### FFT operator

Tobias brought up the need for adding Fourier transform operators to ONNX. The operators are useful for a number
of Deep Learning applications and are already part of frameworks such as 
[PyTorch](https://pytorch.org/docs/stable/fft.html),
[Tensorflow](https://www.tensorflow.org/api_docs/python/tf/signal/fft),
and [Numpy](https://numpy.org/doc/stable/reference/routines.fft.html).

Tobias will reactivate the existing [PR](https://github.com/onnx/onnx/pull/2625/) for these operators.

We discussed the question if should we use complex numbers as inputs/outputs?
Current PR uses two explicit channels for complex numbers.
Rama suggested that with this approach it will be simpler to use different input precisions 
(various sizes of floats, ints).

Evgeny noticed that in TensorFlow models with FFT operations, only those operations use complex numbers.
This suggests that it makes sense to use floating point values as inputs.

We recommended the use of float values to represent real and imaginary parts of complex numbers. 
The last dimension of the input will be of size 2 (real, imaginary).

We also discussed an attribute which could specify that input consists of only real parts of complex
numbers. In this case the last dimension would be of size 1, and the imaginary part of each
number would be assumed to be `0`.

Alternatively, real-valued FFT could be a separate operation. This is an approach used by Numpy.

We will also need to consider adding a special operators which can convert complex numbers to/from the form
used by FFT.

General recommendation is to follow the Numpy, PyTorch and Tensorflow approaches where possible.


### Use of attributes vs. inputs

We discussed the current inconsistency with `Reduce*` operators. 
`ReduceSum` expects `axes` as input, while other `Reduce*` operators expect `axes` to be an attribute.

We decided that we should remove the inconsistency and change all `Reduce*` operators to use `axes` input.

This doesn't change the overall recommendation, which is to prefer attributes over inputs where possible.
