# Thursday, July 28th, 2022 at 9:00am PST

## Agenda

* Infrastructure SIG updates
* [Issue #4343 (Bitwise ops)](https://github.com/onnx/onnx/issues/4343)
* [Issue #4368 (argwhere)](https://github.com/onnx/onnx/issues/4368)
* [PR #4363: 2/4 bit values](https://github.com/onnx/onnx/pull/4363)
* [PR #4351: clarify cast behavior](https://github.com/onnx/onnx/pull/4351)
* [Issue 4322: ScatterElements with min/max reduction](https://github.com/onnx/onnx/issues/4322)
* [Issue 3921: OptionalHasElement/OptionalGetElement extension](https://github.com/onnx/onnx/issues/3921)


## Attendees 

* Alexandre Eichenberger (IBM)
* Ganesan Ramalingam (Microsoft)
* Jacky Chen (Microsoft)
* Liqun Fu (Microsoft)
* Michal Karzynski (Intel)
* Przemyslaw Wysocki (Intel)
* Scott Cyphers (Lightmatter)
* Yuan Yao (Nvidia)

## Notes

### Infrastructure SIG updates

Liqun Fu presented the Infrastructure SIG update. He observed that the existing version-converter
does not handle model-local functions. The ability to apply the version-converter to a function
would be a useful feature. This would allow people authoring a function not to have to define
different function-definitions for different opsets, but use a single function-definition
targetting a specific opset, and use a version-converter to automatically generate a
function-definition for other opset versions.

However, extending the version-converter to handle functions introduces some challenges.
This is because functions tend to be untyped, unlike the main-graph of a model.
Furthermore, functions allow for attribute-references. Hence, transformations that
depend on the specific-value of an attribute of the specific-type of a variable
cannot be realized inside a function-body.

### [Issue #4343 (Bitwise ops)](https://github.com/onnx/onnx/issues/4343)

This issue proposes to add bitwise ops to ONNX. PyTorch and Tensorflow have
a number of bitwise ops, while ONNX lacks them. The Flax BART model is an
example of a model that uses bitwise ops.

Since bitwise scalar ops are common operations in machine instruction sets,
the group agreed that it made sense to add the corresponding elementwise
tensor ops to ONNX. 

It was suggested that a minimal set of ops should be added as primitive ops
(eg., other ops can be encoded in terms of _nand_), and the rest should be
defined as function-ops.

### [Issue #4368 (argwhere)](https://github.com/onnx/onnx/issues/4368)

The author of the issue asks about supporting _argwhere_ as an ONNX op.
It was suggested that example models using the op would help justify making
a case for adding such an op. The author, based on a suggestion, has
use the _NonZero_ op as an alternative substitute. The issue has been
closed.

### [PR #4363: 2/4 bit values](https://github.com/onnx/onnx/pull/4363)

This is a proposal to add new 2/4 bit datatypes to ONNX. The group noted that
the key issue would be the ops that would be needed to support the new datatypes.
Extending all existing ops to support the new types, for example, would be a
very disruptive requirement for most backends/implementations. The group decided
to ask this question in the issue/PR discussion.

### [PR #4351: clarify cast behavior](https://github.com/onnx/onnx/pull/4351)

This PR was noted as ready to be merged.

### [Issue 4322: ScatterElements with min/max reduction](https://github.com/onnx/onnx/issues/4322)

This issue suggests extending the reduction-ops supported by the Scatter ops in ONNX.
One of the discussion authors volunteered (in the online discussion) to create a PR towards
this. The group decided to wait for the PR to review it.

### [Issue 3921: OptionalHasElement/OptionalGetElement extension](https://github.com/onnx/onnx/issues/3921)

This issue suggests an extension to the existing OptionalHasElement and OptionalGetElement ops
to enable defining ops that have optional inputs as a function. This was a preliminary overview
presented to the group, to be followed by a specific PR.