# Thursday, August 25th, 2022 at 9:00am PST

## Agenda

* Infrastructure SIG updates
* PR [#4411](https://github.com/onnx/onnx/pull/4411): Add min/max to Scatter reduction
* PR [#4416](https://github.com/onnx/onnx/pull/4416): SVD op (draft)
* PR [#3948](https://github.com/onnx/onnx/pull/3948): Col2im operator
* PR [#4363](https://github.com/onnx/onnx/pull/4363) : 2/4 bit quantization
* Lower precision float types - presentation by Andrew Fitzgibbon from Graphcore
* ONNX/MLIR serving - Presentation by Qin Yue Chen from IBM 

## Attendees

* Alexandre Eichenberger (IBM)
* Andreas Fehlner (Trumpf Laser)
* Andrew Fitzgibbon (Graphcore)
* Chun-Wei (Jacky) Chen (Microsoft)
* Fei Fei Li
* Ganesan Ramalingam (Microsoft)
* Liqun Fu (Microsoft)
* Martin Croome (GreenWaves Technologies)
* Michal Karzynski (Intel)
* Przemyslaw Wysocki (Intel)
* Qin Yue Chen
* Scott Cyphers (Lightmatter)
* Wei-Sheng Chin (Microsoft)
* Yuan Yao (Nvidia)

## Notes

### Infrastructure SIG updates

Liqun Fu presented the Infrastructure SIG update. 

Jacky Chen is planning to prepare a minor release
after 1.12 to solve a filesystem-related security issue. Relevant pull requests are:
[#4400](https://github.com/onnx/onnx/pull/4400) and [#4470](https://github.com/onnx/onnx/pull/4470).

A proposal was raised with the Infrastructure SIG to add a Java package providing ONNX support
to the main ONNX repository. We would rather not add a Java package to the main ONNX repo, but
we are open to adding a Java package to the ONNX org as a separate repository.


### PR [#4411](https://github.com/onnx/onnx/pull/4411): Add min/max to Scatter reduction

The proposal to add min/max to the reduction attribute of the Scatter op was discussed. The
suggestion is reasonable and the proposal was accepted. The pull request is ready to be merged.
Please review and approve.


### PR [#4416](https://github.com/onnx/onnx/pull/4416): SVD op (draft)

The proposal to add an SVD operator was discussed. The proposal is reasonable and the pull request
is ready to be merged, although it is marked as Draft. Please review and approve.

### PR [#3948](https://github.com/onnx/onnx/pull/3948): Col2im operator

The proposal to add a Col2im operator was discussed. The proposal is reasonable and the pull request
seems complete. Please add any comments if you have some. 
It would be good to have more reviewers for this one.


### PR [#4363](https://github.com/onnx/onnx/pull/4363) : 2/4 bit quantization

PR [#4363](https://github.com/onnx/onnx/pull/4363) contains a fairly long discussion on the
proposal to add 2/4 bit quantization support to ONNX. Please join the discussion and provide
feedback.

The discussion raises the question: do we need to add a 2-bit and 4-bit datatypes to the ONNX IR?
It seems more like an implementation detail for the backends.

Wei-Sheng: we don't need these low-precision details (types) represented in the graph.

Martin Croome: commented that it makes sense to add Quantize/Dequantize operators
with support for a number of different data-widths. However, it is not clear is we would need
to add the datatype to ONNX IR.  We don't want to force the standard to make all operators support 
all quantization levels.


### Lower precision float types - presentation by Andrew Fitzgibbon from Graphcore

Andrew Fitzgibbon from Graphcore presented a proposal to add lower precision float types to ONNX.

Machine learning performance on 8-bit float types is in some cases better than 8-bit integers.

However, we currently don't know if the float-8 layout of bfloat will work best.

Graphcore is talking to all the vendors to see if we can agree on a common layout.

Many questions were raised. The discussion is ongoing.

Should we add these FP8 representations which are not yet widely accepted standards?

Should we add float types of only certain sizes like 8-bit?
Perhapse we should add all types from 1-bit to 7-bit.

Marting Croome: We could consider making a flexible types by specifying signed/unsigned, how many exponent bits,
how many mantissa bits? It would allow people to write custom ops which support custom data types.

Do we need to modify ops to use all these new types? Would they be used only by a subset of ops?

Our recommendation was to create an issue or pull request on GitHub to discuss the proposal.


### ONNX/MLIR serving - Presentation by Qin Yue Chen from IBM 

Qin Yue Chen from IBM presented a proposal to open-source an ONNX serving solution 
based on MLIR to the ONNX organization.

Available solutions to serve ONNX models in the form of a backend API have overheads
which reduce their performance. For example a Java-based solution is available, but
it has a lot of latency (100x more than a native code-compiled solution).

Similarly, available Python solutions need to perform multiple conversions of data (JSON to object in memory, etc.)

ONNX MLIR allows models to be compiled to native code, which is much faster than the other approaches.

We can also reduce API overhead by using GRPC solution. Datasize is smaller, Protocol Buffers can be used directly 
in native code, no data conversion is needed.

The solution developed at IBM has 100x lower latency then the Java-based solution.

IBM proposes to open-source the solution to the ONNX organization. We agreed that it would be a
valuable addition to the ONNX ecosystem.

We discussed the proposal and asked that the current repository be made public so that we can
review the code before accepting the proposal.

We asked the IBM team to review the 
[ONNX community guidelines for new repositories](https://github.com/onnx/onnx/blob/main/community/repo_guidelines.md).
