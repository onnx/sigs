# Thursday, September 29th, 2022 at 9:00am PST

## Agenda

* Infrastructure SIG updates
* Issue [#2248](Masking for LSTM and similar ops)
* PR [#4483](https://github.com/onnx/onnx/pull/4483): Reference implementation of ops and runtime
* PR [#4416](https://github.com/onnx/onnx/pull/4416): SVD op
* PR [#4496](https://github.com/onnx/onnx/pull/4496): Bitwise ops
* PR [#4497](https://github.com/onnx/onnx/pull/4497): Bitwise ops
* PR [#4533](https://github.com/onnx/onnx/pull/4533): Rationalization of LpPool
* PR [#4534](https://github.com/onnx/onnx/pull/4534): Rationalization of MaxPool

## Attendees

* Aaron Bockover (Microsoft)
* Alexandre Eichenberger (IBM)
* Andreas Krebbe (IBM)
* Chun-Wei (Jacky) Chen (Microsoft)
* Ganesan Ramalingam (Microsoft)
* Liqun Fu (Microsoft)
* Michal Karzynski (Intel)
* Przemyslaw Wysocki (Intel)
* Xavier Dupre (Microsoft)
* Yuan Yao (Nvidia)

## Notes

### Infrastructure SIG updates

Jacky is looking into updating protobuf from version 3.16. He is investigating whether
this will be an issue for any ONNX users and tools.

Przemyslaw Wysocki (Intel) has agreed to serve as the release manager for the next version of ONNX.
It is tentatively scheduled for release in December, with a possible code-freeze deadline in November.
Przemyslaw will look into the first steps, and will need to get necessary permissions in the
repo. We will start tagging PRs that meant to be included in the upcoming release.

### Issue [#2248](Masking for LSTM and similar ops)

Andreas resurrected this old issue. Use of masks is common in the the Keras LSTM operation
and supporting it would be helpful. The options discussed included:
* Updating the existing LSTM (and similar ops) to support Mask.
* Defining a new MaskedLSTM op, preferably as a function (composite op).
The second option was considered less disruptive, as it would allow backends
that do not yet implement a masked-variant to use function-expansion as a
way to get a backup implementation (even if potentially less efficient), while
allowing an efficient implementation in the longer run.

A function implementation could be defined either in terms of the existing LSTM
op, or using the Scan op.

### PR [#4483](https://github.com/onnx/onnx/pull/4483): Reference implementation of ops and runtime

The above PR introduces a reference implementation of ONNX ops (in Python),
as well as a reference implementation that executes models/graphs using the
reference implementation of ops, addressing [Issue 4432](https://github.com/onnx/onnx/issues/4432).

There was consensus that such a reference implementation would be greatly
useful as part of the ONNX standard. Since the PR is quite large,
help from the community members is requested in reviewing the PR.

There is the potential that there could be bugs in this implementation. It
was agreed that, in case of a conflict between the (existing) textual specification of the ops and this implementation, the existing textual
specification is to be considered the authoritative version.

There was a discussion about integrating the reference implementation
with the ONNX documentation (eg., via generated documentation) as potential
future work (not part of this PR).

### PR [#4416](https://github.com/onnx/onnx/pull/4416): SVD op

The online discussion of this op in the PR has brought up a question
about whether the spec should clarify the op's relation to a variety
of methods that exist for the SVD op.

Yuan Yao mentioned that linear algebra operations, such as diagonalization,
do have multiple different implementations possible and suggested that a general
approach to this issue would be helpful.

Differences in implementation that affect the efficiency, but not the
output produced, are beyond the scope of ONNX op specifications. ONNX
op specifications do not require a specific implementation to be used
in this case. However,  differences in implementation approaches that
affect the values of the actual output, however, are within the scope
of ONNX op specifications. The ONNX op specification should unambiguously
clarify what the output should be. If supporting more than one implementation
is desirable, then the spec should do this using either a single op,
with attributes to indicate which implementation is required, or
by using more than one op.

In the specific case of SVD, no firm conclusion was reached, as it was
unclear whether there is a demand or need to support multiple different
implementations. Looking at the specific case of scipy/numpy, it supports
only one version of SVD.

### Other items

Due to lack of items, the other agenda items were not discussed much.
A couple of PRs, proposing bit-operations, are ready to be merged.
A couple of other PRs, focused on unifying the various ppoling ops,
are in progress.




