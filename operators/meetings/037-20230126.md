# Thursday, January 26th, 2023 at 9:00am PST

## Agenda

* PR [#4783](https://github.com/onnx/onnx/pull/4783): New op `DeformConv`
* PR [#4793](https://github.com/onnx/onnx/pull/4793): Add `padding_mode` attribute to `Conv`
* PR [#4790](https://github.com/onnx/onnx/pull/4790): Add `dilations` attribute to `AveragePool`
* PR [#4805](https://github.com/onnx/onnx/pull/4805): 8-bit float types
* Issue [#4742](https://github.com/onnx/onnx/issues/4742): Make (uneven) `Split` compatible with Torch
* Should we relax the checker requirement that output type/shape should be specified?


## Attendees

* Aaron Bockover (Microsoft)
* Alexandre Eichenberger (IBM)
* Ashwini Khade (Microsoft)
* Chun-Wei (Jacky) Chen (Microsoft)
* Ganesan Ramalingam (Microsoft)
* George Nash (Intel)
* George Nash (Intel)
* Liqun Fu (Microsoft)
* Lucas Fischer (Dataka Lab)
* Mayank Kaushik (Nvidia)
* Michal Karzynski (Intel)
* Neta Zmora (Nvidia)
* Nick Comly (Nvidia)
* Przemyslaw Wysocki (Intel)
* Rajeev Rao (Nvidia)
* Scott Cyphers (Lightmatter)
* Shubham Bhokare (Microsoft)
* Xavier DuprÃ© (Microsoft)
* Yuan Yao (Nvidia)

## Notes

### PR [#4783](https://github.com/onnx/onnx/pull/4783): New op `DeformConv`

Yuan Yao's PR [#4783](https://github.com/onnx/onnx/pull/4783) introduces the operator Deformable Convolution to ONNX.
The PR is based on papers and a reference implementation from TorchVision.

Please review the PR and add your comments.


### PR [#4793](https://github.com/onnx/onnx/pull/4793): Add `padding_mode` attribute to `Conv` 

Padding modes were discussed in the context of exporting from PyTorch. While available in PyTorch, some modes are not 
available in TensorFlow. Modes may not always be useful, but keeping it consistent with PyTorch was suggested. 
A Python reference implementation should be provided. 
Alternatively non-default padding modes can be added tp the `Pad` operation instead, which can be used before `Conv`. 


### PR [#4790](https://github.com/onnx/onnx/pull/4790): Add `dilations` attribute to `AveragePool`

The unification of pooling operators was discussed. `AveragePool` operation is the only one missing attribute `dilations`. This is the last change needed to make pooling operators consistent.
Przemek Wysocki presented his PR [#4790](https://github.com/onnx/onnx/pull/4790), 
which is ready for review and once finished, the related issue can be closed.


### PR [#4805](https://github.com/onnx/onnx/pull/4805): 8-bit float types

Xavier Dupre has prepared a [PR](https://github.com/onnx/onnx/pull/4805) to add 8-bit float types to ONNX. 
The proposal is to add two types to ONNX, `FLOATE4M3` and `FLOATE5M2`, which are faster and not much
less accurate than other types. NaN support was discussed and while ONNX has infinity, it does not have NaN. 
Some details are implementation specific for now. The PR was reviewed, and it was suggested that it be moved forward.

### Issue [#4742](https://github.com/onnx/onnx/issues/4742): Make (uneven) `Split` compatible with Torch

The current ONNX `Split` operator is not compatible with PyTorch. Compare:

Torch `tensor_split` / `array_split`:

    >>> torch.arange(10).(4)
    (tensor([0, 1, 2]), tensor([3, 4, 5]), tensor([6, 7]), tensor([8, 9]))

ONNX `Split-18`:

    (tensor([0, 1, 2]), tensor([3, 4, 5]), tensor([6, 7, 8]), tensor([9]))

It was suggested that an attribute be added to make Split optionally behave like `tensor_split` in PyTorch.

### Should we relax the checker requirement that output type/shape should be specified?


The possibility of relaxing the requirement for output types/shapes was discussed. 
As type and shape inference is guaranteed for inputs, it was suggested that relaxing the requirement 
for output types/shapes may not have a practical impact on users. 
Some compilers may only work if some shapes are not provided, but ONNX already supports dynamic shapes.

We will make a GitHub issue for further discussion, please contribute your thoughts.


### Opens

Lucas Fischer of Datakalab asked if there was interest in lowering precision beyond int-8
for Quantize/Dequantize. We referenced an [existing PR](https://github.com/onnx/onnx/pull/4363).

Hardware manufacturers are asking if ONNX will support lower bit widths. This is an interesting
proposal, but we need to be careful about the impact on the ecosystem. 
Lucas will review the options and issues and may ask further questions via Slack.
