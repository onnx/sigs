# Thursday, February 2623rd, 2023 at 9:00am PST

## Agenda

* PR [#4783](https://github.com/onnx/onnx/pull/4783): New op `DeformConv`
* PR [#4805](https://github.com/onnx/onnx/pull/4805): 8-bit float types

## Attendees

* Alexandre Eichenberger (IBM)
* Andreas Fehlner (TRUMPF Laser)
* Chun-Wei (Jacky) Chen (Microsoft)
* Ganesan Ramalingam (Microsoft)
* George Nash (Intel)
* Liqun Fu (Microsoft)
* Michal Karzynski (Intel)
* Przemyslaw Wysocki (Intel)
* Rajeev Rao (Nvidia)
* Scott Cyphers (Lightmatter)
* Xavier Dupr√© (Microsoft)
* Yuan Yao (Nvidia)

## Notes

### Issue [#4885: Real model tests](https://github.com/onnx/onnx/issues/4885):

Some models used for testing were previously stored in AWS. These URLs no longer work.
They have been temporarily migrated to another server. We discussed what to do about
these models. Another factor discussed was that these models are large, and
downloading these models can increase the time taken for tests. The models
were not being run to validate the output, but were being used merely to validate
type and shape inference. Hence, it was decided that we would replace them with
smaller versions of the same model by replacing large tensors with dummy data
generated at runtime, which should serve the same purpose for shape inference.
Further, these models are available in the model zoo, for anyone who wants the
actual models.

### Protobuf update

Protobuf 3.21 has breaking changes. Tensorflow has adopted Protobuf 3.21.
Pytorch may be a blocker. ONNX will try to upgrade protobuf in next release.

### IR Version Bump

The next release of ONNX will have a newer IR version. The changes to the IR
in this include
* FunctionProto is updated to allow specification of default-values for
attribute-parameters of the function
* The set of tensor element types will be extended to support 8-bit float.

### Next Release of ONNX (Version 1.14)

We discussed the timeline for the next release of ONNX. It was concluded that
we should aim to have the next release around April/May. Yuan Yao from Nvidia
volunteered to be the release-manager for this release.

### PR [#4783: DeformConv op](https://github.com/onnx/onnx/pull/4783)

There was a question whether the `group` attribute was necessary or if it could be
inferred from the shapes of inputs. Since ONNX's static inference may not have
access to precise input shapes, it was decided that the attribute should be included.
There was a discussion about whether the op spec should be restricted to common cases
(2D, and perhaps 3D) and be generalized to the N-D case. Since `Conv` is defined for
the general N-D case, one reason for making this op N-D is maintaining consistency
between these two ops. On the other hand, implementing the op for the general N-D
case is harder and involves more work (for most backends). If the common usage scenario
is the 2D case, a general implementation may not be needed in the near term.
The consensus was that the op should have a generic name, but the spec should restrict
it to the special-cases (2D or 2D/3D). This allows the same op's spec to be generalized
in the future, if needed.

### PR [#4805: 8-bit float types](https://github.com/onnx/onnx/pull/4805)

This PR introduces 8-bit float types to ONNX and was already discussed in the previous
meeting. The one change from the previous discussion was the move to keep the two
competing families of types (one from Nvidia/Intel/ARM and the other from GraphCore/AMD/Qualcomm)
as distinct types. It was felt that this was better than to leave the behavior undefined
or implementation-dependent (in the special-cases involving infinities and NaNs where the two
specs differ).

### ONNX Roadmap

(Alexandre Eichenberger) The steering committee will be conducting ONNX roadmap sessions
during the month of March, with 3 or 4 sessions of one hour each. Quantization-related
topics will be discussed in particular.

### Specifying target output types for QuantizeLinear and DequantizeLinear

Rajeev Rao brought up the question of extending the Quantize and Dequantize ops
to support 16-bit float types. In particular, one of the question was how to indicate
the desired output type. The two options available are to use an attribute or to
use the input type of one of the inputs.
