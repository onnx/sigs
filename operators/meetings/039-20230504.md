# Thursday, May 4th, 2023 at 9:00am PST

## Agenda

* ONNX release 1.14
* How to contribute to ONNX?
* Proposal to add Gelu op
* Extend GridSample op to N-D
* ZipMap shape-inference

* PR [#4783](https://github.com/onnx/onnx/pull/4783): New op `DeformConv`
* PR [#4805](https://github.com/onnx/onnx/pull/4805): 8-bit float types

## Attendees

* Andreas Fehlner (TRUMPF Laser)
* Chun-Wei (Jacky) Chen (Microsoft)
* Ganesan Ramalingam (Microsoft)
* George Nash (Intel)
* Ken K
* Liqun Fu (Microsoft)
* Michal Karzynski (Intel)
* Philip Lasser
* Scott Cyphers (Lightmatter)
* Soren Lassen (Groq)
* Xavier Dupr√© (Microsoft)
* Yuan Yao (Nvidia)

## Notes

### ONNX Release 1.14

Yuan Yao, the release manager for ONNX release 1.14, indicated that the release was
expected this week (May the 5th).

### How to contribute to ONNX?

Michal gave an overview of how people can contribute to the ONNX standard,
especially to introduce changes to the ONNX standard.

* As an optional first step, proposals for changes and additions can be
made through multiple venues to get initial feedback on an idea. This can
be done through presentations in the annual ONNX Roadmap process, or
by bringing it up for discussion in the SIG meetings (held approximately
every month), or by creating an issue or discussion in the ONNX github repo,
or by discussing it in the Slack channels.

* Make a formal proposal by creating a pull-request in the ONNX repo
  - adhere to its coding conventions, and making changes to the source definitions
  rather than output documents.
  - compile and test your code
  - regenerate output documents
  - create a PR, and have it reviewed
  - once the PR is merged, the change will appear in the next ONNX release,
  which happens approximately 3 or 4 times a year.

More information can be found in:
* https://github.com/onnx/onnx/blob/main/docs/CONTRIBUTING.md
* https://github.com/onnx/onnx/blob/main/CONTRIBUTING.md
* https://github.com/onnx/onnx/blob/main/docs/AddNewOp.md

### Issue [#4933: Gelu op](https://github.com/onnx/onnx/issues/4933):

The Gelu activation function is widely used by models. This issue is a request for
adding Gelu to the ONNX standard. There was agreement in the meeting to add the
op to the standard, as a function op. Two variants of the op exist in practice,
the original version as well as a faster approximation also referred to as FastGelu.
The discussion concluded that supporting both variants made sense, and that
this should be done using an attribute to indicate whether to compute the precise
version or the tanh-based approximation. There was also agreement to make it a
context-dependent function that takes this attribute-value into account.

### PR [#5010: GridSample ND](https://github.com/onnx/onnx/pull/5010)

This PR proposes to extend the 2-dimensional version of GridSample op that exists
in the ONNX standard to general N-dimensions. There was support for the extension.
The PR has also addressed all the comments mentioned in the reviews, and there was
no concern with merging the PR.

Liqun pointed out that an operator to generate a grid would usefully complement the
GridSample op, and volunteered to create a PR for such an op (as a function).

### PR [#5188: ZipMap](https://github.com/onnx/onnx/pull/5188)

This PR fixes a type-and-shape-inference issue involving map types. The one point
discussed was about testing retro-active fixes to older ops such as this, especially
about the need for testing it against an implementation like onnxruntime (which has
served as a reference implementation, especially for ops in the ONNXML domain).
Currently, the testing happens later on, when these changes are integrated into
onnxruntime.

### Specifying the cast behavior for bfloat16

Yuan Yao brought up the issue that the behavior of cast was not clearly specified
in the cast of bfloat16, especially with regards to whether it should use rounding
or truncation when converting higher-precision floats to bfloat16. Other floats
have a precise specification. The recommendation was to determine, and use, the
specification in the bfloat16 standard.