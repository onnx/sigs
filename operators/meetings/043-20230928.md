# Thursday, September 28th, 2023 at 9:00am PST

## Agenda

* Homomorphically encrypted machine learning with ONNX models
* Infrastructure SIG updates
* Deprecating older operator sets: continuing from previous meeting
* [Issue 5466](https://github.com/onnx/onnx/issues/5466): Shape of scale and bias of GroupNormalization
* Opens

## Attendees

* Alexandre Eichenberger (IBM)
* Andreas Fehlner (Trumpf Laser)
* Ganesan Ramalingam (Microsoft)
* George Nash (Intel)
* Liqun Fu (Microsoft)
* Martin Nocker (MCI Management Center Innsbruck)
* Michal Karzynski (Intel)
* Xavier Dupr√© (Microsoft)
* Yuan Yao (Nvidia)


## Homomorphically encrypted machine learning with ONNX models

Martin Nocker presented work related to the paper titled [HE-MAN - Homomorphically Encrypted MAchine learning with oNnx models](https://arxiv.org/abs/2302.08260).

The paper discusses HE-MAN an open-source toolset which allows privacy-preserving inference using encrypted
data with ONNX models without compromising accuracy. The toolset has support for a growing
number of ONNX operations, but currently only relatively simple models can be encrypted and evaluated.
The toolset is based on the TenSEAL homomorphic encryption library.

Martin suggested that we may consider adding homomorphic encryption to the ONNX standard in the future
and opened a discussion on the topic.

### Discussion

Rama suggested that one approach would be to add encrypted types to the ONNX type system in an
analogy to the current quantized types. This leaves open questions of which operations
would need to support the encrypted types and whether encrypted and non-encrypted types
can coexist in the same model.

Xavier suggested that as a POC we could add custom ops to ONNX and custom kernels to ONNX Runtime.

Michal noticed that the HE-MAN toolset demonstrates both the advantages and current 
limitations of applying homomorphic encryption to machine learning.
The toolset is able to perform inference on encrypted data, but only supports a limited number of operations and models.
Moreover, the toolset is tightly coupled to TenSEAL library, and it is unclear if other homomorphic
encryption libraries share the same capabilities and limitations to allow for defining a common standard.

Martin pointed out that homomorphic encryption is a very active research area, however attempts 
to standardize it have started.
One such effort is the [Homomorphic Encryption Standardization](https://homomorphicencryption.org/) consortium.

It was agreed that we should keep an eye on the development of homomorphic encryption and 
consider adding support for it to ONNX in the future.

## Infrastructure SIG updates

ONNX 1.15 release is coming soon. The first [release candidate (RC1)](https://test.pypi.org/project/onnx/1.15.0rc1/)
is currently available for testing. We are asking the community to test the RC1 and report any issues.

The final release is planned for October 10th.

## Deprecating older operator sets

We decided to continue the discussion started during the last meeting to draw conclusions.

Alex noticed that the ONNX version converter does not work correctly on models using the oldest opsets, 
so it may be helpful to remove this code and the operator sets which are broken and unlikely to ever be fixed.

Rama pointed out that newer operator sets are supersets of the previous versions so deprecating an older
operator set would require moving these operators to a new operator set, which is something we
never did before. 

Michal pointed out that operator sets older than 7 are probably not fully supported anywhere 
and support for these old operator sets is unlikely to be added in the future. However,
keeping these operator sets in the ONNX repository is useful for completeness of reference in the standard.

Rama suggested that instead of removing the operator sets we can deprecate them by simply adding an official
recommendation to use a newer operator set. This would allow us to keep the operator documentation in the
ONNX repository for reference, but also make it clear that they are not recommended for use.

Alex pointed out that if we do specify the oldest recommended operator set, we should also make sure that
all models in the ONNX Model Zoo are updated to use this operator set or removed.


## [Issue 5466](https://github.com/onnx/onnx/issues/5466): Shape of scale and bias of GroupNormalization

An issue was raised in the ONNX repository regarding the shape of the `scale` and `bias` tensors
in the `GroupNormalization` operator. The current specification of the operator requires that
these values have `G` entries (number of groups of channels). The issue author
points out that the [Group Normalization paper](https://arxiv.org/abs/1803.08494) as well as the PyTorch implementation
uses a different convention where the `scale` and `bias` tensors have `C` entries (number of channels).

There are currently two implementations of the operation in ONNX Runtime `ai.onnx.GroupNormalization`
and `com.microsoft.GroupNorm` which use the different conventions.

In effect the current ONNX specification is more restrictive than the paper description and implementations.
It was agreed that we should change the ONNX specification to match the implementations.

If needed, we could handle the two different conventions by adding a new attribute to the operator to select which
shape to expect. However, it was agreed that this may not be necessary, and we may simply change the specification
to accept either shape. 

Yuan agreed to work on an update for the specification for the next ONNX release. He will check
if an attribute is needed and will update the specification to accept both shapes.


## Opens

* Please take part in the discussion on [Issue 5622: Treatment of scalar tensors](https://github.com/onnx/onnx/issues/5622)
* ONNX Multi-Device Working Group started work on mechanisms to specify execution of ONNX models across multiple devices
  * [proposal document](https://docs.google.com/document/d/1tikVgXz4huFbkiXDGaRrI0oIV_rgcH-hxsM06TS7J4o/edit?usp=sharing)
  * next [monthly meeting](https://lists.lfaidata.foundation/g/onnx-wg-multidevice/topic/101242394?p=%2C%2C%2C20%2C0%2C0%2C0%3A%3Arelevance%2C%2C%23cal-invite%2C20%2C2%2C0%2C101242394%2Cct%253D1&ct=1) will be held on Oct 11, 2023 05:00 PM Pacific Time (US and Canada)
