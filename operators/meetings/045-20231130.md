# Thursday, November 30th, 2023 at 9:00am PST

## Agenda

* Announcement: December meeting will be cancelled due to holidays
* Proposal to add support for the Int4 data-type to ONNX - [Issue #5776](https://github.com/onnx/onnx/issues/5776)
* Unification of normalization ops 

## Attendees

* Aditya Goel (Quantco)
* Gal Hubara Agam (Nvidia)
* Ganesan Ramalingam (Microsoft)
* George Nash (Intel)
* Justin Chu (Microsoft)
* Michal Karzynski (Intel)
* Neta Zmora (Nvidia)
* Rajeev Rao (Nvidia)
* Ti-Tai Wang (Microsoft)
* Xavier Dupr√© (Microsoft)
* Yuan Yao (Nvidia)

## Proposal to add support for the Int4 data-type to ONNX

Gal Hubara Agam from Nvidia presented the proposal to add support for the Int4 data-type to ONNX. 
The proposal is outlined in [Issue #5776](https://github.com/onnx/onnx/issues/5776).
The proposed data type and changes to ONNX were presented during the meeting.

Large language models (LLMs) are showing great performance on various tasks, but large model sizes
increase hardware requirements for serving (memory size) and slow down token generation (memory bandwidth).

The Int4 datatype is gaining traction for LLM model compression, it reduces memory bandwidth and total memory requirements.
Several implementations of this approach have been developed, including AutoGPTQ, solutions by HuggingFace, 
Intel's NeuralCompressor, Microsoft's DeepSpeed Compression, and Nvidia's TRT-LLM.

Int4 is a sub-byte data type, it would be the first of its kind for ONNX, which introduces a new approach
to data handling. Int4 data can be packed, with two elements fitting into a single byte, or four into a 16-bit type.
The Int4 data must be unpacked prior to computation and then repacked once the computation is complete.
This process is also required for certain shape operations, such as transpose.

### Blocked Quantization

Int4 quantization may require the use of blocked quantization to attain satisfactory accuracy. 
In this approach, the quantization axis is segmented into blocks, each assigned its unique quantization scale.
Block size becomes an additional parameter for quantization and dequantization (Q/DQ) operations, 
along with scale and zero-point. Scale and zero-point are represented as a two-dimensional tensor,
with one value per block per quantization axis element. Typical block sizes are 32, 64, 128, and 256.

### Proposed Changes to ONNX

The proposed changes to ONNX include the addition of a new data type, Int4, and support for blocked quantization.

The Int4 data type would be added as a custom non-Numpy type, similar to bfloat16 and float8 types. Additional 
functionality should be added to support packing and unpacking of int4. Support for the new data type should be added
to relevant operators, including QuantizeLinear, DequantizeLinear, Shape, Size, Transpose, Reshape and Constant.

Additionally support for blocked quantization should be added to QuantizeLinear and DequantizeLinear operators. 
The block size and number of blocks could be inferred from the scale tensor dimensions.

A few open issues were identified during the presentation, in particular relating to the packing of data with 
odd dimensions. If the innermost dimension size is not even, packing must involve the addition of
padding values, which must be removed when unpacking. This would require additional bookkeeping and operations.
A few approaches to this issue were suggested.

### Discussion

The panel asked if other types of the MXFP family would also be added in the future. Gal responded that the current plan
is to only add support for Int4, as it is the one with proven good results. However, other sub-byte integer types
could be added as well.

Xavier suggested that the current process of adding data types to ONNX involves a lot of work. He proposed
that for experimental purposes we could add the ability to specify local types, defined within the model.
This would involve specifying a custom type with a description including precision, handling of mantissa and
exponent (for floating point types), and a function to convert the type to a basic float and back.
Custom types would be specified by the creator of the model and would not be part of the ONNX standard, but could
be supported by developers as they experiment with new data types. 
 

## Unification of normalization ops

Yuan Yao raised the issue that we currently have multiple normalization operators in ONNX, but
not all of these operations have the same sets of attributes and behaviours. Yuan suggested that we could
unify these operators into a single normalization operator, with the type of normalization specified as an attribute.

During the discussion it was agreed that we should unify the normalization operators available in ONNX.
We did not reach a consensus as to replacing the current operators with a single operator, as this
would create a large group of deprecated operators needed for backward compatibility. The idea of
a single normalization operator does have merits, but we would need to consider the impact on existing models.
