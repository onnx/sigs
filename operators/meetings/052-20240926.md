# ONNX Operators SIG meeting
# Thursday, September 26th, 2024 at 9:00am PST

## Agenda

* Upcoming Release 1.17
* [PR 6395](https://github.com/onnx/onnx/pull/6395): Clarify `Clip` behavior
* [Issue 6103](https://github.com/onnx/onnx/issues/6103): Behavior of `ReduceSumSquare`
* Adding Large Language Model (LLM) operators to ONNX

## Attendees

* Ganesan Ramalingam (Microsoft)
* George Nash (Intel)
* Michal Karzy≈Ñski (Intel)
* Raghavan Naresh Sarangapani (Intel)
* Shubham Bhokare (Microsoft)

## Meeting Minutes

### Upcoming Release 1.17

Raghavan Naresh updated the team on the 1.17 release candidate status.
The release candidate has been tested, including integration with ONNX Runtime.
There are still some issues open, but the overall release is progressing,
with final steps expected to be completed shortly and the release is due next week.


### Clarification on Clip behavior

The behavior of the clip operator when the `min` input value is greater than `max` is not explicitly addressed
in the ONNX specification. [PR 6395](https://github.com/onnx/onnx/pull/6395) was created to clarify this behavior
and align it with the behavior of Numpy's clip function.
The proposed behavior is to set all input values to the maximum value.
It was agreed that the proposed behavior is a good addition and the PR should be merged.

### Clarification on ReduceSumSquare behavior

The reduce sum square operation was clarified for cases where no reduction axes are specified and the
`noop_with_empty_axes` attribute is set to 1.
We currently have a discrepancy between the specification, which states that in this case,
"the output tensor would be equivalent to input tensor" and the reference implementation which
returns the square of the input tensor.
We agreed that the correct behavior is to compute the square of the input tensor
without performing any reduction and to update the specification accordingly.

### Adding LLM related operators

Shubham Bhokare presented his work and plans to add new operators to support Large Language Models (LLMs) in ONNX.

The proposed operators include:

* RMSNormalization (`com.microsoft.SkipSimplifiedLayerNormalization`)
* SkipLayerNormaliztion
* SkipRMSNormalization (`com.microsoft.SkipSimplifiedLayerNormalization`)
* RotaryEmbedding (`com.microsoft.RotaryEmbedding`)
* GroupQueryAttention (`com.microsoft.GroupQueryAttention`)
* MatMulNBits (`com.microsoft.MatMulNBits`)
* SparseAttention (`com.microsoft.SparseAttention`)

The `com.microsoft` specifications are based on the Microsoft implementation of these operators in ONNX Runtime.
They can be found in the [ONNX Runtime docs](https://github.com/microsoft/onnxruntime/blob/bb0c1f0a05a9dad747a78ed0c47d0ae5f6df8ef0/docs/ContribOperators.md).

Work is already in progress for the first four operators (`RMSNormalization`, `SkipLayerNormalization`,
`SkipRMSNormalization`, and `RotaryEmbedding`). A pull request is expected to be submitted soon.

George Nash is working on implementing the `MatMulInBits` operator and will coordinate with Shubham to avoid overlap.
The discussion focused on standardizing variable bit-width quantization and unpacking schemes for the `MatMulInBits`
operator, which performs matrix multiplication with weights quantized at 2 to 7 bits.
While most of the functionality can be implemented as an ONNX function, a helper operator may be required
to dequantize and unpack the weights.
