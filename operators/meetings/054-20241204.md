# ONNX Operators SIG meeting
# Wednesday, December 4th, 2024 at 9:00am PST

## Agenda

* Architecture/Infrastructure SIG Lead Change
* [PR 6531](https://github.com/onnx/onnx/pull/6531): Scale-type in `QuantizeLinear` and `DequantizeLinear`
* [PR 6500](https://github.com/onnx/onnx/pull/6500): `MatMulNBits` operator
* LLM-related operations
  * [PR 6443](https://github.com/onnx/onnx/pull/6443): `RMSNormalization`, `SkipLayerNormalization`, `SkipLayerNormalization`
  * [PR 6461](https://github.com/onnx/onnx/pull/6461): `RotaryEmbedding` 
  * [PR 6501](https://github.com/onnx/onnx/pull/6501): `ScalarDotProductAttention`
* Random Number Generation Discussion
* Next Release Planning

## Attendees

* Ganesan Ramalingam (Microsoft)
* Liqun Fu (Microsoft)
* Yuan Yao (NVIDIA)
* George Nash (Intel)
* Michał Karzyński (Intel)
* Adam Pocock (Oracle)
* Shubham Bhokare (Microsoft)
* Andreas Fehlner (TRUMPF)

## Meeting Minutes

### Architecture/Infrastructure SIG Lead Change

During the meeting, Liqun Fu announced his decision to step down from his role as Architecture/Infrastructure
SIG lead, citing increased commitments in other areas. A proposal was put forward to have Xavier Dupré
take over as the new lead, though the team emphasized they would welcome additional volunteers to
serve as co-leads. The team expressed their gratitude to Liqun for his service and contributions.
There was also a broader call for more volunteers to help with PR reviews, as this remains a key 
need for the group.

### Scale-type in QuantizeLinear and DequantizeLinear PR Review

Yuan Yao presented [PR 6531](https://github.com/onnx/onnx/pull/6531), which proposes allowing different
scale types in QDQ operators. During the discussion, concerns were raised about the reference implementation's
use of numpy dtypes instead of ONNX data types, and the team agreed to align with the ONNX data types.
The team also identified the need to ensure consistent handling of zero values between 
the specification and implementation. George Nash volunteered to review the PR further.

### MatMulNBits PR Discussion 

George Nash presented [PR 6500](https://github.com/onnx/onnx/pull/6500) for the `MatMulNBits` operator,
which includes several modifications from the contrib implementation in ONNX Runtime. Key changes include 
making N and K attributes optional since they can be inferred from shapes, adding default
values for bits and block size, removing the group index input, and removing int32 support 
for the data blob. The operator is currently being used in the `llama-2-7b` model available 
on HuggingFace. During the discussion, the team explored whether the functionality could be 
expressed using existing QDQ operators. It was also decided to enhance the proposal by 
adding float16 support for the scale type.

### LLM Operators Review

Shubham Bhokare provided an update on three LLM-related PRs, with the discussion focusing 
particularly on precision control for the `ScalarDotProductAttention` operator. Yuan Yao agreed 
to provide specific suggestions for precision attributes. The team also discussed nomenclature, 
with a suggestion to simplify the name from `ScalarDotProductAttention` to simply `Attention` 
since the operator covers a broader range of functionality.

### Random Number Generation Discussion

Adam Pocock presented a proposal for implementing deterministic random number generation (RNG)
in ONNX, drawing inspiration from JAX's splittable RNG approach. The proposal focused on enabling
both reproducible results and efficient parallel execution. The team discussed how this approach
has proven successful in JAX and other systems, though there were questions about its potential
impact on graph parallelization. While the benefits for reproducibility were clear, the team
concluded that further discussion would be needed to work through the implementation details.

### Next Release Planning

Plans for the next ONNX release were discussed, targeting February/March 2025. 
NVIDIA will be taking the lead on release management this time. The primary focus will be incorporating
the new LLM operators. The team also noted the need to address any outstanding Windows-related
issues before the release.
