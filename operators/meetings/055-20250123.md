# ONNX Operators SIG Meeting
# Thursday, January 23rd, 2025 at 9:00am PST

## Agenda

* Architecture/Infrastructure SIG Co-chair Selection
* ONNX 1.18 Release Planning
* [PR 6500](https://github.com/onnx/onnx/pull/6500) `MatMulNBits` Operator Discussion
* LLM-related Operators
  * [PR 6501](https://github.com/onnx/onnx/pull/6501) `Attention`
  * [PR 6461](https://github.com/onnx/onnx/pull/6461) `RotaryEmbedding`
  * [PR 6443](https://github.com/onnx/onnx/pull/6443) `Normalization`
* Open Discussion


## Attendees

* Alexandre Eichenberger (IBM)
* Ganesan Ramalingam (Microsoft)
* George Nash (Intel)
* Justin Chu (Microsoft)
* Michał Karzyński (Intel)
* Shubham Bhokare (Microsoft)
* Sunny Anand (IBM)
* Xavier Dupré (Microsoft)
* Yuan Yao (NVIDIA)

## Meeting Minutes

### Architecture/Infrastructure SIG updates

Xavier Dupré volunteered to act as a co-chair of the Architecture/Infrastructure SIG and
Andreas Fehlner volunteered to act a second co-chair. The team agreed to move forward with this proposal,
and a PR will be submitted to formalize these leadership changes.

### Next ONNX Release Planning

ONNX 1.18 release is targeted for release in March 2025. Krishna from AMD has volunteered to
be the release manager, marking AMD's first time in this role.
The proposed code freeze date is mid-to-late February, with a focus on adding PRs related to
operator updates and LLM-related enhancements.

### `MatMulNBits` Operator Discussion

George Nash presented updates on the `MatMulNBits` operator. Concerns were raised about whether it should be 
included in the ONNX standard as its functionality could be achieved using the QDQ pattern. At this point, 
it is not certain whether we would gain functionality by adding this operator as a function or whether it should 
remain a contrib operator and local function. This type of operators could serve as a hint to backend 
implementors for which DQD patterns are more common and would benefit from fusion and quantization support.

### Attention-related Operators

Shubham Bhokare noted that his `RotaryEmbedding` ([PR 6461](https://github.com/onnx/onnx/pull/6461)), 
and `Normalization` ([PR 6443](https://github.com/onnx/onnx/pull/6443)) PRs are ready for final review.
He also brought up for discussion the format of the Query, Key and Value input to the Attention operator.
Currently they can be represented as either a 4D tensor with shape (batch_size, num_heads, sequence_length, head_size) 
or 3D tensor with shape (batch_size, sequence_length, hidden_size), where hidden_size = num_heads * head_size.
This adds complexity to the ONNX specification, but does map onto implementations from multiple frameworks.

Yuan Yao mentioned that the operator could have more granular control of precision by providing separate 
attributes for the precision or matmul and softmax operations. He also mentioned that requiring all the inputs 
to be the same type is too restrictive.


### Open discussion

* Multi-Device execution support: https://github.com/onnx/onnx/pull/664
* Bug with models larger than 2GB: https://github.com/onnx/onnx/issues/6529
