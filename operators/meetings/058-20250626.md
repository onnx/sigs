# ONNX Operators SIG meeting
# Thursday, June 26th, 2025 at 9:00am PST

## Attendees

* Ganesan Ramalingam (Microsoft)
* Justin Chu (Microsoft)
* Michał Karzyński (Intel)
* Shubham Bhokare (Microsoft)
* Yuan Yao (NVIDIA)

## Agenda

* [PR 7030](https://github.com/onnx/onnx/pull/7030): `FLOAT8E8M0` Support
* GenAI Working Group Updates
* [Issue #7071](https://github.com/onnx/onnx/issues/7071): Mixed precision `Pow` op
* [Issue #7072](https://github.com/onnx/onnx/issues/7072): Accumulation precision in `MatMul`
* Opens

## Meeting Minutes

### `FLOAT8E8M0` Support

Yuan presented [PR 7030](https://github.com/onnx/onnx/pull/7030) introducing the `FLOAT8E8M0` data type and associated
new version of the `Cast` operation. E8M0 serves as the common scale type for 
[microscaling (MX) formats](https://www.opencompute.org/documents/ocp-microscaling-formats-mx-v1-0-spec-final-pdf).
An important consideration, brought up in the meeting is the rounding behavior differences between libraries:

* CUDA experiments show **round-up** preserves accuracy better ([paper](https://arxiv.org/abs/2506.08027)).
* PyTorch and ml_dtypes currently default to **round-to-nearest**.
* TorchAO supports multiple rounding modes but defaults to **round-down**.

Yuan's PR adds `rounding_mode` attribute (up, down, nearest) to the `Cast` operator. The group agreed that supporting 
multiple rounding modes and defaulting to round-up is appropriate.

### GenAI Working Group Updates

The GenAI Working Group is concentrating on two major efforts related to the `Attention` operation: 
to allow in-place KV cache updates and to add more flexible precision attributes to subgraphs. 

The current `Attention` op concatenates new values to the KV cache and input and output shapes
change at each generation step. The proposal is to use in-place updates of a pre-allocated KV cache
and base all shapes on maximal sequence length. Additional parameters will indicate the currently active
region for processing. This should optimize performance by avoiding memory reallocation. A new tensor-update
operator, similar to the current `Scatter`, will be needed for in-place writes.

The second area under development is allowing the use of different precisions and quantization patterns
in different parts of attention (matmul, softmax, value multiplication). Two approaches are under discussion:
(a) extend the `Attention` op with extra attributes (precision, scale, zero-point) or (b) allow each part of 
the attention calculation to be defined using subgraph attributes. 

The second approach is very flexible, as it would allow plugging in 2–6 (2 internal + up to 4 for I/O) small subgraphs
which can perform casts, quantize/dequantize, etc. This would be a general-purpose solution similar to PyTorch's 
Flex Attention. However, the increased complexity may make implementations costly.

There are additional discussions in the working group related to the idea of representing precompiled (binary) 
subgraphs within ONNX models. This would allow backends to store optimized subgraphs in an executable format
specific to a backend. The group is also coordinating required work in the PyTorch ONNX exporter and 
efforts to support exporting HuggingFace static caches to ONNX.

### Mixed precision `Pow` op

The ONNX `Pow` operator was modified to allow the base and exponent to use different data types, but the result 
currently matches the type of the base. This creates ambiguity in cases like Pow(int(2), float(0.5)), where 
the specification is unclear. Multiple interpretations are possible: (a) cast the exponent to int and compute using 
integer arithmetic, (b) cast the base to float, compute as a float, and cast the result back to int, or 
(c) change the return type to float entirely and compute in floating point.

In the meeting, participants agreed that usage-wise, (c) is the most intuitive, followed by (b), 
with (a) being least useful. However, some existing implementations already assume (a). 
For now, the group decided to clarify the spec using (b), aligning with ONNX Runtime behavior, 
while documenting the ambiguity and recommending explicit casts in models. 
In a future opset bump we may adopt (c) or to disallow a mix of precisions entirely.

### Accumulation precision in `MatMul`

Current `MatMul` op does not define its accumulation precision, which can cause inconsistent results 
(e.g. with 16-bit floats). The accumulator precision could be specified with a new attribute to guarantee
the precision of the result. As a first step we may choose to document a recommended usage pattern: 
to cast inputs to 32-bit before MatMul and cast results back. This pattern can be fused by backends.

### Opens

Justin Chu brought up pull requests which require input and reviews:

* [PR #7075](https://github.com/onnx/onnx/pull/7075): Cumulative Product Operator
* [PR #7084](https://github.com/onnx/onnx/pull/7084): Add BitCast operator
* [PR #7085](https://github.com/onnx/onnx/pull/7085): Update float8 table for the Cast op spec
